<?xml version="1.0"?>
<yandex>
    <!-- YieldSensei ClickHouse Configuration -->
    <!-- Optimized for high-frequency DeFi time-series data -->

    <!-- Logging Configuration -->
    <logger>
        <level>information</level>
        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
        <size>1000M</size>
        <count>10</count>
        <console>1</console>
    </logger>

    <!-- HTTP Server Configuration -->
    <http_port>8123</http_port>
    <tcp_port>9000</tcp_port>
    <interserver_http_port>9009</interserver_http_port>

    <!-- Listen Configuration -->
    <listen_host>0.0.0.0</listen_host>
    <listen_host>::</listen_host>

    <!-- Memory and Performance Configuration -->
    <max_server_memory_usage>3221225472</max_server_memory_usage> <!-- 3GB -->
    <max_concurrent_queries>1000</max_concurrent_queries>
    <max_connections>4096</max_connections>
    <keep_alive_timeout>30</keep_alive_timeout>
    
    <!-- Query Execution Limits -->
    <max_query_size>268435456</max_query_size> <!-- 256MB -->
    <interactive_delay>1000000</interactive_delay>
    
    <!-- Memory Usage Optimization for DeFi Analytics -->
    <uncompressed_cache_size>8589934592</uncompressed_cache_size> <!-- 8GB -->
    <mark_cache_size>5368709120</mark_cache_size> <!-- 5GB -->
    <compiled_expression_cache_size>134217728</compiled_expression_cache_size> <!-- 128MB -->
    
    <!-- Index Cache for Fast Queries -->
    <index_uncompressed_cache_size>1073741824</index_uncompressed_cache_size> <!-- 1GB -->
    <index_mark_cache_size>1073741824</index_mark_cache_size> <!-- 1GB -->

    <!-- Background Processing for MergeTree Engines -->
    <background_pool_size>16</background_pool_size>
    <background_merges_mutations_concurrency_ratio>2</background_merges_mutations_concurrency_ratio>
    <background_schedule_pool_size>16</background_schedule_pool_size>
    <background_message_broker_schedule_pool_size>16</background_message_broker_schedule_pool_size>
    
    <!-- I/O Optimization for Time-Series Data -->
    <max_table_size_to_drop>0</max_table_size_to_drop>
    <max_partition_size_to_drop>0</max_partition_size_to_drop>
    
    <!-- Merge Tree Settings Optimized for DeFi -->
    <merge_tree>
        <max_suspicious_broken_parts>5</max_suspicious_broken_parts>
        <parts_to_delay_insert>150</parts_to_delay_insert>
        <parts_to_throw_insert>300</parts_to_throw_insert>
        <max_delay_to_insert>1</max_delay_to_insert>
        <max_parts_in_total>100000</max_parts_in_total>
        <replicated_deduplication_window>100</replicated_deduplication_window>
        <replicated_deduplication_window_seconds>604800</replicated_deduplication_window_seconds>
        <max_replicated_merges_in_queue>1000</max_replicated_merges_in_queue>
        <number_of_free_entries_in_pool_to_lower_max_size_of_merge>8</number_of_free_entries_in_pool_to_lower_max_size_of_merge>
        <max_bytes_to_merge_at_min_space_in_pool>1048576</max_bytes_to_merge_at_min_space_in_pool>
        <max_bytes_to_merge_at_max_space_in_pool>161061273600</max_bytes_to_merge_at_max_space_in_pool>
        
        <!-- Compression Settings for Market Data -->
        <min_merge_bytes_to_use_direct_io>10737418240</min_merge_bytes_to_use_direct_io> <!-- 10GB -->
        <merge_with_ttl_timeout>86400</merge_with_ttl_timeout>
        <write_final_mark>1</write_final_mark>
        <merge_with_recompression_ttl_timeout>86400</merge_with_recompression_ttl_timeout>
        <try_fetch_recompressed_part_timeout>7200</try_fetch_recompressed_part_timeout>
        
        <!-- Index Granularity Optimization -->
        <max_suspicious_broken_parts_bytes>1073741824</max_suspicious_broken_parts_bytes> <!-- 1GB -->
    </merge_tree>

    <!-- Path Configuration -->
    <path>/var/lib/clickhouse/</path>
    <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>
    <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>
    <format_schema_path>/var/lib/clickhouse/format_schemas/</format_schema_path>

    <!-- User Access Management -->
    <users_config>users.xml</users_config>
    <access_control_path>/var/lib/clickhouse/access/</access_control_path>

    <!-- Default Profile for Time-Series Queries -->
    <default_profile>default</default_profile>
    <default_database>yieldsensei</default_database>

    <!-- Timezone Configuration -->
    <timezone>UTC</timezone>

    <!-- Query Log Configuration for Performance Analysis -->
    <query_log>
        <database>system</database>
        <table>query_log</table>
        <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 8192</engine>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </query_log>

    <!-- Query Thread Log for Detailed Analysis -->
    <query_thread_log>
        <database>system</database>
        <table>query_thread_log</table>
        <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 8192</engine>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </query_thread_log>

    <!-- Query Views Log for Materialized View Performance -->
    <query_views_log>
        <database>system</database>
        <table>query_views_log</table>
        <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 8192</engine>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </query_views_log>

    <!-- Part Log for Merge Analysis -->
    <part_log>
        <database>system</database>
        <table>part_log</table>
        <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 8192</engine>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </part_log>

    <!-- Metric Log for Resource Monitoring -->
    <metric_log>
        <database>system</database>
        <table>metric_log</table>
        <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 8192</engine>
        <flush_interval_milliseconds>1000</flush_interval_milliseconds>
        <collect_interval_milliseconds>1000</collect_interval_milliseconds>
    </metric_log>

    <!-- Asynchronous Metric Log -->
    <asynchronous_metric_log>
        <database>system</database>
        <table>asynchronous_metric_log</table>
        <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 8192</engine>
        <flush_interval_milliseconds>60000</flush_interval_milliseconds>
    </asynchronous_metric_log>

    <!-- Crash Log -->
    <crash_log>
        <database>system</database>
        <table>crash_log</table>
        <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 8192</engine>
        <flush_interval_milliseconds>1000</flush_interval_milliseconds>
    </crash_log>

    <!-- Session Log for Security Monitoring -->
    <session_log>
        <database>system</database>
        <table>session_log</table>
        <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 8192</engine>
        <flush_interval_milliseconds>1000</flush_interval_milliseconds>
    </session_log>

    <!-- Network Compression for Reduced Bandwidth -->
    <compression>
        <case>
            <method>lz4</method>
        </case>
    </compression>

    <!-- Built-in Configuration -->
    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>

    <!-- Maximum Dictionary Size -->
    <max_dictionary_size>1000000</max_dictionary_size>

    <!-- Distributed DDL Configuration -->
    <distributed_ddl>
        <path>/clickhouse/task_queue/ddl</path>
    </distributed_ddl>

    <!-- Mark Cache Policy -->
    <mark_cache_policy>SLRU</mark_cache_policy>

    <!-- Uncompressed Cache Policy -->
    <uncompressed_cache_policy>SLRU</uncompressed_cache_policy>

    <!-- Format Schema -->
    <format_schema_path>/var/lib/clickhouse/format_schemas/</format_schema_path>

    <!-- Custom Settings for DeFi Analytics -->
    <profiles>
        <default>
            <!-- Memory Usage -->
            <max_memory_usage>10000000000</max_memory_usage> <!-- 10GB -->
            <use_uncompressed_cache>1</use_uncompressed_cache>
            <load_balancing>random</load_balancing>
            
            <!-- Query Execution -->
            <max_execution_time>300</max_execution_time> <!-- 5 minutes -->
            <max_rows_to_read>1000000000</max_rows_to_read> <!-- 1B rows -->
            <max_bytes_to_read>100000000000</max_bytes_to_read> <!-- 100GB -->
            
            <!-- Result Limits -->
            <max_result_rows>1000000</max_result_rows> <!-- 1M rows -->
            <max_result_bytes>1000000000</max_result_bytes> <!-- 1GB -->
            
            <!-- Join Configuration for Protocol Analysis -->
            <max_bytes_in_join>10000000000</max_bytes_in_join> <!-- 10GB -->
            <join_use_nulls>0</join_use_nulls>
            <join_algorithm>hash</join_algorithm>
            
            <!-- Aggregation Optimization -->
            <max_bytes_before_external_group_by>20000000000</max_bytes_before_external_group_by> <!-- 20GB -->
            <max_bytes_before_external_sort>20000000000</max_bytes_before_external_sort> <!-- 20GB -->
            
            <!-- Insert Settings for High-Frequency Data -->
            <insert_quorum>0</insert_quorum>
            <insert_quorum_timeout>60000</insert_quorum_timeout>
            <insert_deduplicate>1</insert_deduplicate>
            <deduplicate_blocks_in_dependent_materialized_views>0</deduplicate_blocks_in_dependent_materialized_views>
            
            <!-- Network Timeout -->
            <receive_timeout>300</receive_timeout>
            <send_timeout>300</send_timeout>
            
            <!-- HTTP Configuration -->
            <http_connection_timeout>2</http_connection_timeout>
            <http_send_timeout>30</http_send_timeout>
            <http_receive_timeout>30</http_receive_timeout>
            
            <!-- Output Format -->
            <output_format_write_statistics>1</output_format_write_statistics>
            <output_format_json_quote_64bit_integers>0</output_format_json_quote_64bit_integers>
            <output_format_json_quote_denormals>0</output_format_json_quote_denormals>
            
            <!-- Read Settings for Time-Series Scans -->
            <read_in_order_two_level_merge_threshold>100</read_in_order_two_level_merge_threshold>
            <prefer_localhost_replica>1</prefer_localhost_replica>
            
            <!-- Distributed Queries -->
            <distributed_product_mode>deny</distributed_product_mode>
            <distributed_aggregation_memory_efficient>1</distributed_aggregation_memory_efficient>
            
            <!-- Merge Tree Read Settings -->
            <max_streams_to_max_threads_ratio>1</max_streams_to_max_threads_ratio>
            <merge_tree_max_rows_to_use_cache>128000000</merge_tree_max_rows_to_use_cache>
            <merge_tree_max_bytes_to_use_cache>2013265920</merge_tree_max_bytes_to_use_cache>
            
            <!-- Optimize for Time-Series Queries -->
            <allow_experimental_analyzer>1</allow_experimental_analyzer>
            <optimize_move_to_prewhere>1</optimize_move_to_prewhere>
            <optimize_move_to_prewhere_if_final>0</optimize_move_to_prewhere_if_final>
            
            <!-- Parallel Processing -->
            <max_threads>8</max_threads>
            <max_insert_threads>4</max_insert_threads>
            <max_final_threads>4</max_final_threads>
            
            <!-- Index Usage -->
            <use_index_for_in_with_subqueries>1</use_index_for_in_with_subqueries>
            <joined_subquery_requires_alias>0</joined_subquery_requires_alias>
        </default>
        
        <!-- High Performance Profile for Analytics -->
        <analytics>
            <max_memory_usage>20000000000</max_memory_usage> <!-- 20GB -->
            <max_execution_time>1800</max_execution_time> <!-- 30 minutes -->
            <max_threads>16</max_threads>
            <max_bytes_before_external_group_by>50000000000</max_bytes_before_external_group_by> <!-- 50GB -->
            <max_bytes_before_external_sort>50000000000</max_bytes_before_external_sort> <!-- 50GB -->
        </analytics>
        
        <!-- Fast Insert Profile for High-Frequency Data -->
        <fast_insert>
            <max_memory_usage>5000000000</max_memory_usage> <!-- 5GB -->
            <max_execution_time>60</max_execution_time> <!-- 1 minute -->
            <max_threads>4</max_threads>
            <insert_deduplicate>0</insert_deduplicate>
            <async_insert>1</async_insert>
            <wait_for_async_insert>0</wait_for_async_insert>
        </fast_insert>
    </profiles>

    <!-- Quotas for Resource Management -->
    <quotas>
        <default>
            <interval>
                <duration>3600</duration>
                <queries>0</queries>
                <errors>0</errors>
                <result_rows>0</result_rows>
                <read_rows>0</read_rows>
                <execution_time>0</execution_time>
            </interval>
        </default>
    </quotas>

    <!-- Include Custom Settings -->
    <include_from>/etc/clickhouse-server/config.d/custom-settings.xml</include_from>
</yandex> 